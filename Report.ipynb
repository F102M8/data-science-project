{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5dd5a3f-8a3d-4268-ac9b-a0957a2c4b11",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkBlue\"> Project Report: Steam Games Data Analysis</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422597ac-1b5c-406e-953c-99b47cc2a55d",
   "metadata": {},
   "source": [
    "## <span style=\"color:Purple\"> A) Web Scraping </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36207f-3555-4245-8106-5ba69f23e5d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Introduction\n",
    "The goal of this part is to gather data on approximately 3000 games from the website [SteamDB](https://steamdb.info). The data collected includes various attributes of the games, which will be used for subsequent analysis, preprocessing, feature engineering, and model training.\n",
    "\n",
    "### 2. Methodology\n",
    "\n",
    "#### a. Initial Data Collection\n",
    "1. **Target URL**: [SteamDB](https://steamdb.info)\n",
    "2. **Objective**: Extract URLs of about 3000 games and gather detailed information for each game.\n",
    "3. **Data Points Collected**:\n",
    "    - NAME\n",
    "    - STORE_GENRE\n",
    "    - RATING_SCORE\n",
    "    - N_SUPPORTED_LANGUAGES\n",
    "    - DEVELOPERS\n",
    "    - SUPPORTED_PLATFORMS\n",
    "    - POSITIVE_REVIEWS\n",
    "    - NEGATIVE_REVIEWS\n",
    "    - TECHNOLOGIES\n",
    "    - RELEASE_DATE\n",
    "    - TOTAL_TWITCH_PEAK\n",
    "    - PRICE\n",
    "    - N_DLC\n",
    "    - 24_HOUR_PEAK\n",
    "\n",
    "#### b. Scraping Process\n",
    "1. **Scripts and Notebooks**:\n",
    "    - `weScrape_1.ipynb`: This script extracts game URLs and gathers detailed information for each game.\n",
    "    - `weScrape_2_price.ipynb`: This script ensures that the prices of games are converted to USD, addressing the challenge of varied pricing in different currencies.\n",
    "2. **Files Generated**:\n",
    "    - `game_urls.txt`: List of URLs for the 3000 games.\n",
    "    - `games_info.csv`: Contains detailed information about the games.\n",
    "    - `games_details.csv`: Contains game names, release dates, and prices in USD.\n",
    "\n",
    "#### c. Challenges Faced\n",
    "1. **Time-Consuming Process**: Scraping a large number of games required significant time and computational resources. Multiple systems were used to expedite the process.\n",
    "2. **Price Standardization**: Different currencies on the SteamDB website posed a challenge, which was addressed by writing a separate script to ensure all prices are in USD.\n",
    "\n",
    "### 3. Results\n",
    "The web scraping phase successfully gathered data for 3000 games, resulting in two primary datasets:\n",
    "1. `games_info.csv`: Comprehensive details about each game.\n",
    "2. `games_details.csv`: Focused on name, release date, and standardized price.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139e405-8146-45ea-995c-a1661f8ddadf",
   "metadata": {},
   "source": [
    "## <span style=\"color:Purple\">B) Preprocessing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47cb71-84e4-4691-add7-2731eb06b242",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "### 1. Introduction\r\n",
    "\r\n",
    "The preprocessing phase is crucial for ensuring that the data is clean, consistent, and ready for subsequent analysis. This phase involves handling missing values, correcting data types, creating new features, and addressing any inconsistencies. The preprocessing steps were executed in the `Preprocessing.ipynb` notebook, with `game_info.csv` and `game_details.csv` as inputs, resulting in the output file `preprocessed_game_info.csv`.\r\n",
    "\r\n",
    "### 2. Methodology\r\n",
    "\r\n",
    "#### a. Loading Data\r\n",
    "\r\n",
    "The datasets `game_info.csv` and `game_details.csv` were loaded into Pandas DataFrames to facilitate data manipulation. This step sets the foundation for all subsequent data processing tasks.\r\n",
    "\r\n",
    "#### b. Initial Data Exploration\r\n",
    "\r\n",
    "Basic exploratory data analysis was performed to understand the structure and content of the datasets. This included checking the data types of each column, identifying missing values, and generating summary statistics to get a high-level overview of the data.\r\n",
    "\r\n",
    "#### c. Removing Duplicated Games\r\n",
    "\r\n",
    "Duplicates in the dataset can lead to skewed analysis results. To prevent this, duplicate entries based on the 'NAME' column were identified and removed.\r\n",
    "\r\n",
    "#### d. Handling Missing Values\r\n",
    "\r\n",
    "Several steps were taken to handle missing values across different columns:\r\n",
    "\r\n",
    "- **General Replacement**: 'N/A' values in the dataset were standardized by replacing them with Pandas' `pd.NA`.\r\n",
    "- **'DEVELOPERS' Column**: To maintain data integrity, rows with missing values in the 'DEVELOPERS' column were dropped.\r\n",
    "- **'RELEASE_DATE' Column**: The 'RELEASE_DATE' column was processed to extract the year, creating a new 'PUBLISH_YEAR' column. Missing years were filled with the median value of the column, and the original 'RELEASE_DATE' column was subsequently dropped.\r\n",
    "- **'PRICE' Column**: Missing prices were filled using corresponding values from the `game_details.csv` file. Prices were cleaned and converted to numeric values, ensuring consistency across the dataset. Remaining rows with missing prices were dropped to avoid incomplete data entries.\r\n",
    "- **'N_SUPPORTED_LANGUAGES' Column**: Missing values were filled with a default value, and the column was cleaned and converted to an integer type to ensure proper data formatting.\r\n",
    "- **'RATING_SCORE' Column**: Missing values were filled with a placeholder, cleaned, and converted to numeric values. The placeholder values were then replaced with the mean rating score to ensure a realistic representation of the data.\r\n",
    "\r\n",
    "#### e. Data Cleaning and Feature Engineering\r\n",
    "\r\n",
    "Several columns required specific cleaning and feature engineering steps:\r\n",
    "\r\n",
    "- **'STORE_GENRE' Column**: Missing values were filled, unnecessary text was removed, and the genres were split into a list format to facilitate better analysis.\r\n",
    "- **'24_HOUR_PEAK' Column**: This column was cleaned by filling missing values and extracting relevant numerical values, which were then converted to an integer type.\r\n",
    "- **'TECHNOLOGIES' Column**: Missing values were filled with empty strings, and the technologies were split into lists for better usability during analysis.\r\n",
    "- **'TOTAL_TWITCH_PEAK' Column**: The column was split into 'TWITCH_PEAK_HOUR' and 'TWITCH_PEAK_YEAR', both of which were cleaned and converted to numeric types. The original 'TOTAL_TWITCH_PEAK' column was dropped after extracting the necessary information.\r\n",
    "- **'N_DLC' Column**: Missing values were temporarily filled and then restored. The percentage of null values was calculated to make an informed decision, and the column was dropped due to a high percentage of missing values.\r\n",
    "- **'TOTAL_REVIEW' Column**: A new column was created to represent the proportion of positive reviews out of the total reviews, providing insights into the overall sentiment and reception of the games.\r\n",
    "\r\n",
    "### 3. Data Integration\r\n",
    "\r\n",
    "The cleaned and preprocessed data from the primary dataset (`game_info.csv`) and the secondary dataset (`game_details.csv`) were merged to form a unified dataset. This step ensured that all relevant information was combined, providing a comprehensive dataset for further analysis.\r\n",
    "\r\n",
    "### 4. Final Dataset Preparation\r\n",
    "\r\n",
    "A final check was conducted to ensure all preprocessing steps were correctly applied. This included verifying that there were no remaining missing values, that all data types were correct, and that all necessary transformations had been completed. The cleaned and prepared dataset was then saved as `preprocessed_game_info.csv`.\r\n",
    "\r\n",
    "### 5. Summary\r\n",
    "\r\n",
    "In summary, the preprocessing phase involved:\r\n",
    "- Loading and exploring the data.\r\n",
    "- Removing duplicate entries.\r\n",
    "- Handling missing values across various columns.\r\n",
    "- Cleaning data and engineering new features.\r\n",
    "- Integrating datasets to form a comprehensive dataset.\r\n",
    "\r\n",
    "These steps ensured that the data was clean, consistent, and ready for subsequent analysis and modeling. The output of this phase, `preprocessed_game_info.csv`, y additional details or specific adjustments, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021203a2-ef8d-4a30-b5e4-da94a0218fd7",
   "metadata": {},
   "source": [
    "## <span style=\"color:Purple\">C) Analysis</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d472cc9-d122-4b4a-ae6f-46c44cb87dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
